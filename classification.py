# -*- coding: utf-8 -*-
"""Classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wYqseBw-5pCI6yCZUJ9ZUH8j6JnUmoEE
"""

!pip install scikit-learn

!pip install numpy

from sklearn.datasets import fetch_openml

mnist = fetch_openml('mnist_784', version = 1)
mnist.keys()

X, y = mnist['data'], mnist['target']
X.shape , y.shape

import matplotlib.pyplot as plt
import matplotlib as mltp
import numpy as np

some_pic=X.loc[[0]]
image_array = np.array(some_pic)

image_array

some_image = image_array.reshape(28,28)

plt.imshow(some_image , cmap = "binary")

y[0]

y = y.astype(np.uint8)
y[0]

X[:5], X.shape

x_train , x_test, y_train , y_test = X[:60000], X[60000:], y[:60000], y[60000:]
x_train

"""## lets try to simlify the problem to a binary problem and train on this . if the number is five (true) else false![download.png](data:image/png;base64,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)"""

y_train_5 = (y_train == 5)
y_train_5

y_test_5 = (y_test == 5)

from sklearn.linear_model import SGDClassifier

sgd_clf = SGDClassifier(random_state= 42)
sgd_clf.fit(x_train,y_train_5)

sgd_clf.predict(X.loc[[0]])

"""# Performance Measure

## **cross_validation**

creating cross validation ourself
"""

from sklearn.model_selection import StratifiedKFold
from sklearn.base import clone

"""# THIS splits the training set into 3 folds:

This parameter specifies that the data should be split into 3 folds. In the context of cross-validation, this means that the data will be divided into three groups. For each iteration of cross-validation, two of these groups will be used for training, and the third will be used for validation/testing.
"""

skfolds = StratifiedKFold(n_splits = 3, random_state= 42, shuffle=True)

"""Iteration 1:


Test Set: Fold 1


Training Set: Fold 2 and Fold 3


Iteration 2:


Test Set: Fold 2


Training Set: Fold 1 and Fold 3


Iteration 3:


Test Set: Fold 3


Training Set: Fold 1 and Fold 2


"""

for train_index, test_index in skfolds.split(x_train,y_train_5):
  clone_clf = clone(sgd_clf)

  x_train_folds = x_train[train_index]
  y_train_folds = y_train_5[train_index]
  x_test_folds = x_train[test_index]
  y_test_folds = y_train_5[test_index]
  clone_clf.fit(x_train_folds, y_train_folds)
  y_pred = clone_clf.predict(x_test_folds)
  n_correct = sum(y_pred == y_test_folds)
  print(n_correct/len(y_pred))

"""## using sklearn cross_val_score"""

from sklearn.model_selection import cross_val_score

cross_val_score(sgd_clf,x_train , y_train_5, cv=3 ,scoring='accuracy')

"""## now let's look at classifier that just clssifies every single image in the "not_5" class"""

from sklearn.base import BaseEstimator

from ast import Return
class Never5classifier(BaseEstimator):
  def fit(self,X, y=None):
    return self
  def predict(self,X):
    return np.zeros((len(X),1), dtype=bool)

never_5_clf = Never5classifier()
cross_val_score(never_5_clf,x_train,y_train_5,cv=3,scoring= 'accuracy')

"""this is simply because just 10% of the images are 5s, so if you always guess that an image is not 5, you will be right 90% of the time. thats why accuracy is not a good performance measure esp when we have a skewed dataset"""



"""## **Confusion Matrix**"""

from sklearn.model_selection import cross_val_predict
y_train_pred = cross_val_predict(sgd_clf,x_train, y_train_5,cv=3)
y_train_pred

"""## like cross_val_score function , cross_val_predict performs k folds cross validation but instead of returning the score it returns the predicted values"""

from sklearn.metrics import confusion_matrix
confusion_matrix(y_train_5,y_train_pred)

y_train_perfect_predict = y_train_5
confusion_matrix(y_train_5, y_train_perfect_predict)

from sklearn.metrics import precision_score, recall_score
  precision_score(y_train_5,y_train_pred)

recall_score(y_train_5,y_train_pred) # the percentage says how many of predictions it said is it wasn't 5 but it was five

from sklearn.metrics import f1_score
f1_score(y_train_5,y_train_pred)

# Convert DataFrame to NumPy array
some_pic_array = some_pic.to_numpy()

# Flatten the image
flattened_pic = some_pic_array.flatten()

# Reshape to 2D array with one row
reshaped_pic = flattened_pic.reshape(1, -1)

# Get the decision function scores
y_scores = sgd_clf.decision_function(reshaped_pic)
y_scores

threshold = 0
y_some_pic_pred = (y_scores > threshold)
y_some_pic_pred

# the sgdclassifier uses a threshhold equal to zero so the previous code returns true, now let's raise the threshhold
threshold = 200000
y_some_pic_pred = (y_scores > threshold)
y_some_pic_pred

# this means by increasing threshold the recall will decrease, false negative will increase
y_scores = cross_val_predict(sgd_clf,x_train, y_train_5,cv=3,method='decision_function')

from sklearn.metrics import precision_recall_curve
# with these scores use precision_recall_curve to compute precision and recall for all possible threshhold
precisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores)

def plot_precision_recall_vs_threshold(precisions, recalls, thresholds):
    plt.plot(thresholds, precisions[:-1], "b--", label="Precision")
    plt.plot(thresholds, recalls[:-1], "g-", label="Recall")

plot_precision_recall_vs_threshold(precisions, recalls, thresholds)
plt.show()

#another way to select a good precision and recall is to plot precision  directly against recall
#np.argmax  with at least 90% precision will give you the first index of maximum value, which in this case means the first true value
threshold_90_precision = thresholds[np.argmax(precisions >= 0.90)]
y_train_90_pred = (y_scores >= threshold_90_precision)

precision_score(y_train_5,y_train_90_pred)

recall_score(y_train_5,y_train_90_pred)

# we can create a classifier with virtually any precision you want

"""## the ROC curve

is used with binary classifier, it's very similar to precision , recall curve

but instead of plotting precision versus recall , it plots the true positive rate (another name for recall)  against the false positive rate (FPR) , The true negative rate.
"""

from sklearn.metrics import roc_curve
fpr, tpr, thresholds = roc_curve(y_train_5, y_scores)

def plot_roc_curve(fpr, tpr, label=None):
    plt.plot(fpr, tpr, linewidth=2, label=label)
    plt.plot([0, 1], [0, 1], 'k--')

plot_roc_curve(fpr, tpr)
plt.show()

"""## a good classifier stays as far away from the dotted line as possible (top left corner)
##another way to compare classifiers is to measure the area under the curve (AUC). A perfect classifier will have a ROC AUC equal to 1, whereas a purely random classifier will have a ROC AUC equal to 0.5.
"""

from sklearn.metrics import roc_auc_score
roc_auc_score(y_train_5, y_scores)

# now let's train a Randomforest classifier to compare it with SGDclassifier
from sklearn.ensemble import RandomForestClassifier
forest_clf = RandomForestClassifier(random_state=42)
y_probas_forest = cross_val_predict(forest_clf,x_train, y_train_5,cv=3,method='predict_proba')

y_probas_forest

#roc_curve function expects labels and scores, but let's use positive class's probability as the score
y_scores_forest = y_probas_forest[:, 1] # score = proba of positive class
fpr_forest, tpr_forest, thresholds_forest = roc_curve(y_train_5,y_scores_forest)

plt.plot(fpr, tpr, "b:", label="SGD")
plot_roc_curve(fpr_forest, tpr_forest, "Random Forest")
plt.legend(loc="lower right")
plt.show()

# random forest did better

""" multiclassification methods:


1. SGD classifier
2.  RandomForestcLASSIFIER
3.  Naive Bayes

binary classifier


1. logisitc regression
2.   support vector machines



 we can use binary classification to predict more than two class with multiple binary classifiers
"""

#scikit learn detects when you try to use binary classification algo for a multi class classification and it automatically runs OvR or OvO depending on the algo
from sklearn.svm import SVC
svm_clf = SVC()
svm_clf.fit(x_train,y_train)

svm_clf.predict([flattened_pic])

# it trained OVO , 45 binary classifiers, got their decision score for the image and selected the class that won the most duels

some_digit_scores = svm_clf.decision_function([flattened_pic])
some_digit_scores

np.argmax(some_digit_scores)

svm_clf.classes_

svm_clf.classes_[5]

# when a classifier is trained, it scores the list of target classes in its classes_ attribute, ordered by value

# if you want to force scikitlearn to use ovo or ovr you can use OneVsOneClassifier or OneVsRestClassifier
# from sklearn.multiclass import OneVsRestClassifier
# ovr_clf = OneVsRestClassifier(SVC())
# ovr_clf.fit(x_train,y_train)
# ovr_clf.predict([flattened_pic])

sgd_clf.fit(x_train,y_train)
sgd_clf.predict([flattened_pic])

sgd_clf.decision_function([flattened_pic])

# we can use cross val score to evaluate the classifier accuracy
cross_val_score(sgd_clf, x_train, y_train, cv=3, scoring="accuracy")

# by scaling the inputs we can get better results
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
x_train_scaled = scaler.fit_transform(x_train.astype(np.float64))
cross_val_score(sgd_clf, x_train_scaled, y_train, cv=3, scoring="accuracy")

"""#Error analysis

one way to analyse the model is to analyze the types of error it makes

first we look at the confusion matrix .
"""

y_train_pred = cross_val_predict(sgd_clf,x_train_scaled, y_train,cv=3)

conf_mx = confusion_matrix(y_train,y_train_pred)
conf_mx

"""let's focus on the plot on the errors. first we need to divide each value in the confusion matrix by the number of images in the corresponding  class so that you can compare error rates instead of absolute number of errors"""

row_sums = conf_mx.sum(axis=1, keepdims=True)
norm_conf_mx = conf_mx / row_sums

np.fill_diagonal(norm_conf_mx, 0)
plt.matshow(norm_conf_mx, cmap=plt.cm.gray)
plt.show()

"""the col for class 8 is quite bright telling us that many images get misclassified as 8s. however the row for class is not that bad

## Multilabel Classifier/KNN
"""

from sklearn.neighbors import KNeighborsClassifier
y_train_large = (y_train >= 7)
y_train_odd = (y_train % 2 == 1)
y_multilabel = np.c_[y_train_large, y_train_odd]
knn_clf = KNeighborsClassifier()
knn_clf.fit(x_train,y_multilabel)

knn_clf.predict([some_pic])

y_train_knn_pred = cross_val_predict(knn_clf,x_train, y_train,cv=3)
f1_score(y_train,y_train_knn_pred,average='macro')

"""## multioutput classification

the last type of classification is called multioutput classification. it is simply a generalization of multilabel class where each label can be multiclass (it can have more than two possible values)

let's start by creating a training and test sets by taking MNIST images and adding noise to their pixel intensities
"""

noise = np.random.randint(0,100,(len(x_train),784))
x_train_mod = x_train + noise
noise = np.random.randint(0,100,(len(x_test),784))
x_test_mod = x_test + noise

knn_clf.fit(x_train_mod,y_train)
clean_digit = knn_clf.predict([x_test_mod[0]])

from sklearn.datasets import fetch_openml
import numpy as np

mnist = fetch_openml('mnist_784', version = 1)
mnist.keys()

X, y = mnist["data"], mnist["target"]
y = y.astype(np.int32)

import matplotlib.pyplot as plt
import matplotlib as mltp
import numpy as np

import tensorflow as tf
from sklearn.preprocessing import OneHotEncoder
X = tf.constant(X)
y = tf.constant(y)
X.shape , y.shape
X = tf.reshape(X,[70000,28,28])
X = tf.cast(X,tf.float32)

X  = tf.random.shuffle(
    X, seed=42
)
X = X/255.0

y = tf.one_hot(y, depth=10)
X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]
import pandas as pd

def create_model(my_learning_rate):


  model = tf.keras.models.Sequential()

  model.add(tf.keras.layers.Flatten(input_shape=[28, 28]))

  model.add(tf.keras.layers.Dense(units=300, activation='relu'))

  model.add(tf.keras.layers.Dense(units=10, activation='softmax'))
  model.compile(optimizer=tf.keras.optimizers.Adam(my_learning_rate),
                loss="sparse_categorical_crossentropy",
                metrics=['accuracy'])

  return model



def train_model(model, train_features, train_label, epochs,
                batch_size=None, validation_split=0.1):
  """Train the model by feeding it data."""

  history = model.fit(x=train_features, y=train_label, batch_size=batch_size,
                      epochs=epochs, shuffle=True,
                      validation_split=validation_split)

  # To track the progression of training, gather a snapshot
  # of the model's metrics at each epoch.
  epochs = history.epoch
  hist = pd.DataFrame(history.history)

  return epochs, hist

def plot_curve(epochs, hist, list_of_metrics):
  """Plot a curve of one or more classification metrics vs. epoch."""
  # list_of_metrics should be one of the names shown in:
  # https://www.tensorflow.org/tutorials/structured_data/imbalanced_data#define_the_model_and_metrics

  plt.figure()
  plt.xlabel("Epoch")
  plt.ylabel("Value")

  for m in list_of_metrics:
    x = hist[m]
    plt.plot(epochs[1:], x[1:], label=m)

  plt.legend()

print("Loaded the plot_curve function.")